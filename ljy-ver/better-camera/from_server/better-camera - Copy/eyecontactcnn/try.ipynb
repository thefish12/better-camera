{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“eye-contact-env (Python 3.6.13)”的单元格需要ipykernel包。\n",
      "\u001b[1;31m运行以下命令，将 \"ipykernel\" 安装到 Python 环境中。\n",
      "\u001b[1;31m命令: \"conda install -n eye-contact-env ipykernel --update-deps --force-reinstall\""
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import argparse, os, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model import model_static\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from colour import Color\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--video', type=str, help='input video path. live cam is used when not specified')\n",
    "parser.add_argument('--face', type=str, help='face detection file path. dlib face detector is used when not specified')\n",
    "parser.add_argument('--model_weight', type=str, help='path to model weights file', default='data/model_weights.pkl')\n",
    "parser.add_argument('--jitter', type=int, help='jitter bbox n times, and average results', default=0)\n",
    "parser.add_argument('-save_vis', help='saves output as video', action='store_true')\n",
    "parser.add_argument('-save_text', help='saves output as text', action='store_true')\n",
    "parser.add_argument('-display_off', help='do not display frames', action='store_true')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "CNN_FACE_MODEL = 'data/mmod_human_face_detector.dat' # from http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
    "\n",
    "\n",
    "def bbox_jitter(bbox_left, bbox_top, bbox_right, bbox_bottom):\n",
    "    cx = (bbox_right+bbox_left)/2.0\n",
    "    cy = (bbox_bottom+bbox_top)/2.0\n",
    "    scale = random.uniform(0.8, 1.2)\n",
    "    bbox_right = (bbox_right-cx)*scale + cx\n",
    "    bbox_left = (bbox_left-cx)*scale + cx\n",
    "    bbox_top = (bbox_top-cy)*scale + cy\n",
    "    bbox_bottom = (bbox_bottom-cy)*scale + cy\n",
    "    return bbox_left, bbox_top, bbox_right, bbox_bottom\n",
    "\n",
    "\n",
    "def drawrect(drawcontext, xy, outline=None, width=0):\n",
    "    (x1, y1), (x2, y2) = xy\n",
    "    points = (x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)\n",
    "    drawcontext.line(points, fill=outline, width=width)\n",
    "\n",
    "\n",
    "def run(video_path, face_path, model_weight, jitter, vis, display_off, save_text):\n",
    "    # set up vis settings\n",
    "    red = Color(\"red\")\n",
    "    colors = list(red.range_to(Color(\"green\"),10))\n",
    "    font = ImageFont.truetype(\"data/arial.ttf\", 40)\n",
    "\n",
    "    # set up video source\n",
    "    if video_path is None:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        video_path = 'live.avi'\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # set up output file\n",
    "    if save_text:\n",
    "        outtext_name = os.path.basename(video_path).replace('.avi','_output.txt')\n",
    "        f = open(outtext_name, \"w\")\n",
    "    if vis:\n",
    "        outvis_name = os.path.basename(video_path).replace('.avi','_output.avi')\n",
    "        imwidth = int(cap.get(3)); imheight = int(cap.get(4))\n",
    "        outvid = cv2.VideoWriter(outvis_name,cv2.VideoWriter_fourcc('M','J','P','G'), cap.get(5), (imwidth,imheight))\n",
    "\n",
    "    # set up face detection mode\n",
    "    if face_path is None:\n",
    "        facemode = 'DLIB'\n",
    "    else:\n",
    "        facemode = 'GIVEN'\n",
    "        column_names = ['frame', 'left', 'top', 'right', 'bottom']\n",
    "        df = pd.read_csv(face_path, names=column_names, index_col=0)\n",
    "        df['left'] -= (df['right']-df['left'])*0.2\n",
    "        df['right'] += (df['right']-df['left'])*0.2\n",
    "        df['top'] -= (df['bottom']-df['top'])*0.1\n",
    "        df['bottom'] += (df['bottom']-df['top'])*0.1\n",
    "        df['left'] = df['left'].astype('int')\n",
    "        df['top'] = df['top'].astype('int')\n",
    "        df['right'] = df['right'].astype('int')\n",
    "        df['bottom'] = df['bottom'].astype('int')\n",
    "\n",
    "    if (cap.isOpened()== False):\n",
    "        print(\"Error opening video stream or file\")\n",
    "        exit()\n",
    "\n",
    "    if facemode == 'DLIB':\n",
    "        cnn_face_detector = dlib.cnn_face_detection_model_v1(CNN_FACE_MODEL)\n",
    "    frame_cnt = 0\n",
    "\n",
    "    # set up data transformation\n",
    "    test_transforms = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    # load model weights\n",
    "    model = model_static(model_weight)\n",
    "    model_dict = model.state_dict()\n",
    "    snapshot = torch.load(model_weight)\n",
    "    model_dict.update(snapshot)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    model.cuda()\n",
    "    model.train(False)\n",
    "\n",
    "    # video reading loop\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        print(\"readed\")\n",
    "        if ret == True:\n",
    "            height, width, channels = frame.shape\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            frame_cnt += 1\n",
    "            bbox = []\n",
    "            if facemode == 'DLIB':\n",
    "                dets = cnn_face_detector(frame, 1)\n",
    "                for d in dets:\n",
    "                    l = d.rect.left()\n",
    "                    r = d.rect.right()\n",
    "                    t = d.rect.top()\n",
    "                    b = d.rect.bottom()\n",
    "                    # expand a bit\n",
    "                    l -= (r-l)*0.2\n",
    "                    r += (r-l)*0.2\n",
    "                    t -= (b-t)*0.2\n",
    "                    b += (b-t)*0.2\n",
    "                    bbox.append([l,t,r,b])\n",
    "            elif facemode == 'GIVEN':\n",
    "                if frame_cnt in df.index:\n",
    "                    bbox.append([df.loc[frame_cnt,'left'],df.loc[frame_cnt,'top'],df.loc[frame_cnt,'right'],df.loc[frame_cnt,'bottom']])\n",
    "\n",
    "            frame = Image.fromarray(frame)\n",
    "            for b in bbox:\n",
    "                face = frame.crop((b))\n",
    "                img = test_transforms(face)\n",
    "                img.unsqueeze_(0)\n",
    "                if jitter > 0:\n",
    "                    for i in range(jitter):\n",
    "                        bj_left, bj_top, bj_right, bj_bottom = bbox_jitter(b[0], b[1], b[2], b[3])\n",
    "                        bj = [bj_left, bj_top, bj_right, bj_bottom]\n",
    "                        facej = frame.crop((bj))\n",
    "                        img_jittered = test_transforms(facej)\n",
    "                        img_jittered.unsqueeze_(0)\n",
    "                        img = torch.cat([img, img_jittered])\n",
    "\n",
    "                # forward pass\n",
    "                output = model(img.cuda())\n",
    "                if jitter > 0:\n",
    "                    output = torch.mean(output, 0)\n",
    "                score = F.sigmoid(output).item()\n",
    "\n",
    "                coloridx = min(int(round(score*10)),9)\n",
    "                draw = ImageDraw.Draw(frame)\n",
    "                drawrect(draw, [(b[0], b[1]), (b[2], b[3])], outline=colors[coloridx].hex, width=5)\n",
    "                draw.text((b[0],b[3]), str(round(score,2)), fill=(255,255,255,128), font=font)\n",
    "                if save_text:\n",
    "                    f.write(\"%d,%f\\n\"%(frame_cnt,score))\n",
    "\n",
    "            if not display_off:\n",
    "                frame = np.asarray(frame) # convert PIL image back to opencv format for faster display\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                cv2.imshow('114',frame)\n",
    "                print(\"showing\")\n",
    "                if vis:\n",
    "                    outvid.write(frame)\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if vis:\n",
    "        outvid.release()\n",
    "    if save_text:\n",
    "        f.close()\n",
    "    cap.release()\n",
    "    print('DONE!')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(args.video, args.face, args.model_weight, args.jitter, args.save_vis, args.display_off, args.save_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eye-contact-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
